#!/usr/bin/env python3
"""
Visualize Transformer attention patterns on ECG signals.

Usage:
    python scripts/visualize_attention.py --checkpoint checkpoints_cv/fold_0/best.ckpt
"""

import argparse
import sys
from pathlib import Path
import yaml

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sys.path.insert(0, str(Path(__file__).parent.parent))

from models import create_ecg_transformer, BrugadaClassifier
from data import DataConfig, AugmentationConfig, BrugadaDataModule


def load_config(config_path: Path) -> dict:
    """Load YAML configuration."""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def plot_attention_heatmap(attention_weights, layer_idx, head_idx, save_path=None):
    """
    Plot attention heatmap for a specific layer and head.
    
    Args:
        attention_weights: (batch, num_heads, seq_len, seq_len)
        layer_idx: Which transformer layer
        head_idx: Which attention head
        save_path: Where to save the plot
    """
    # Extract single sample, single head
    attn = attention_weights[0, head_idx].cpu().numpy()  # (seq_len, seq_len)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attn,
        cmap='viridis',
        xticklabels=False,
        yticklabels=False,
        cbar_kws={'label': 'Attention Weight'}
    )
    plt.title(f'Attention Pattern - Layer {layer_idx + 1}, Head {head_idx + 1}')
    plt.xlabel('Key Position (ECG Patches)')
    plt.ylabel('Query Position (ECG Patches)')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Saved: {save_path}")
    else:
        plt.show()
    
    plt.close()


def plot_attention_rollout(attention_weights, save_path=None):
    """
    Compute and plot attention rollout (cumulative attention across layers).
    
    Shows which input patches the CLS token attends to most strongly.
    """
    # Start with identity matrix
    batch_size, num_heads, seq_len, _ = attention_weights[0].shape
    
    # Average attention heads
    attn_layers = [attn.mean(dim=1) for attn in attention_weights]  # List of (batch, seq_len, seq_len)
    
    # Compute rollout (cumulative attention)
    rollout = attn_layers[0]
    for layer_attn in attn_layers[1:]:
        rollout = torch.matmul(layer_attn, rollout)
    
    # Extract CLS token attention to all patches
    cls_attention = rollout[0, 0, 1:].cpu().numpy()  # Skip CLS token itself
    
    # Plot
    plt.figure(figsize=(14, 4))
    plt.bar(range(len(cls_attention)), cls_attention)
    plt.xlabel('ECG Patch Index')
    plt.ylabel('Cumulative Attention Weight')
    plt.title('Attention Rollout: Which ECG Patches Does the Model Focus On?')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Saved: {save_path}")
    else:
        plt.show()
    
    plt.close()
    
    return cls_attention


def plot_ecg_with_attention(ecg_signal, cls_attention, save_path=None):
    """
    Plot ECG signal overlaid with attention weights.
    
    Shows which parts of the ECG the model focuses on.
    
    Args:
        ecg_signal: (num_leads, signal_length)
        cls_attention: (num_patches,) - attention weights for each patch
        save_path: Where to save
    """
    num_leads, signal_length = ecg_signal.shape
    patch_size = signal_length // len(cls_attention)
    
    fig, axes = plt.subplots(3, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    lead_names = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
    
    for lead_idx in range(min(12, num_leads)):
        ax = axes[lead_idx]
        
        # Plot ECG signal
        time = np.arange(signal_length) / 500  # Assuming 500 Hz
        ax.plot(time, ecg_signal[lead_idx], 'k-', linewidth=0.5, alpha=0.7)
        
        # Overlay attention as background shading
        for patch_idx, attn_weight in enumerate(cls_attention):
            start_idx = patch_idx * patch_size
            end_idx = (patch_idx + 1) * patch_size
            
            ax.axvspan(
                time[start_idx], 
                time[min(end_idx, signal_length - 1)],
                alpha=attn_weight * 0.5,  # Scale for visibility
                color='red'
            )
        
        ax.set_title(f'Lead {lead_names[lead_idx]}')
        ax.set_xlabel('Time (s)')
        ax.set_ylabel('Amplitude (mV)')
        ax.grid(alpha=0.3)
    
    plt.suptitle('ECG with Attention Overlay (Red = High Attention)', fontsize=16)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Saved: {save_path}")
    else:
        plt.show()
    
    plt.close()


def visualize_sample(
    model,
    sample,
    output_dir: Path,
    sample_idx: int
):
    """
    Visualize attention for a single ECG sample.
    
    Args:
        model: Trained ECGTransformer
        sample: ECGSample object
        output_dir: Where to save visualizations
        sample_idx: Index for file naming
    """
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Prepare input
    signal = sample.signal.unsqueeze(0)  # Add batch dimension
    label = sample.label.item()
    patient_id = sample.patient_id
    
    # Forward pass with attention
    model.eval()
    with torch.no_grad():
        logits, attention_weights = model(signal, return_attention=True)
        prob = torch.sigmoid(logits).item()
    
    print(f"\nSample {sample_idx} (Patient {patient_id}):")
    print(f"  True Label: {'Brugada' if label == 1 else 'Normal'}")
    print(f"  Predicted: {'Brugada' if prob > 0.5 else 'Normal'} (prob={prob:.3f})")
    print(f"  Num Layers: {len(attention_weights)}")
    print(f"  Attention Shape: {attention_weights[0].shape}")
    
    # 1. Plot attention heatmaps for each layer (first head only)
    for layer_idx, attn in enumerate(attention_weights):
        plot_attention_heatmap(
            attn,
            layer_idx=layer_idx,
            head_idx=0,
            save_path=output_dir / f"sample_{sample_idx}_layer_{layer_idx}_head_0.png"
        )
    
    # 2. Plot attention rollout
    cls_attention = plot_attention_rollout(
        attention_weights,
        save_path=output_dir / f"sample_{sample_idx}_rollout.png"
    )
    
    # 3. Plot ECG with attention overlay
    ecg_signal = signal[0].cpu().numpy()  # (num_leads, signal_length)
    plot_ecg_with_attention(
        ecg_signal,
        cls_attention,
        save_path=output_dir / f"sample_{sample_idx}_ecg_with_attention.png"
    )
    
    return prob, attention_weights


def main():
    parser = argparse.ArgumentParser(description='Visualize Transformer attention on ECG')
    parser.add_argument('--checkpoint', type=Path, required=True)
    parser.add_argument('--training-config', type=Path, default=Path('config/train.yaml'))
    parser.add_argument('--data-config', type=Path, default=Path('config/data.yaml'))
    parser.add_argument('--num-samples', type=int, default=5)
    parser.add_argument('--output-dir', type=Path, default=Path('attention_visualizations'))
    parser.add_argument('--brugada-only', action='store_true')
    args = parser.parse_args()
        
    print("=" * 80)
    print("ATTENTION VISUALIZATION")
    print("=" * 80)
    
    # Load training config to reconstruct model
    print("\n[1/5] Loading training configuration...")
    training_config = load_config(args.training_config)
    
    # Load data config
    print("\n[2/5] Loading data configuration...")
    data_config_dict = load_config(args.data_config)
    data_config = DataConfig(**data_config_dict)
    
    # Create augmentation config (no augmentation for visualization)
    aug_params = data_config_dict.get('augmentation', {})
    augmentation_config = AugmentationConfig(
        amplitude_scale_range=tuple(aug_params.get('amplitude_scale', {}).get('range', [0.8, 1.2])),
        amplitude_scale_prob=0.0,  # No augmentation for viz
        noise_std=aug_params.get('noise', {}).get('std', 0.05),
        noise_prob=0.0,
        baseline_wander_amplitude=aug_params.get('baseline_wander', {}).get('amplitude', 0.1),
        baseline_wander_frequency=tuple(aug_params.get('baseline_wander', {}).get('frequency_range', [0.1, 0.5])),
        baseline_wander_prob=0.0,
        time_warp_sigma=aug_params.get('time_warp', {}).get('sigma', 0.2),
        time_warp_knots=aug_params.get('time_warp', {}).get('knots', 4),
        time_warp_prob=0.0,
        lead_scale_range=tuple(aug_params.get('lead_scale', {}).get('range', [0.9, 1.1])),
        lead_scale_prob=0.0,
    )
    
    # Load data
    print("\n[3/5] Loading data...")
    datamodule = BrugadaDataModule(
        config=data_config,
        augmentation_config=augmentation_config
    )
    datamodule.setup(stage='test')
    
    # Reconstruct base model using training config
    print("\n[4/5] Loading model...")
    model_config = training_config['model']
    architecture = model_config['architecture']
    
    if architecture == 'transformer':
        from models import create_ecg_transformer
        base_model = create_ecg_transformer(
            model_size=model_config['size'],
            in_channels=12,
            num_classes=1,
            dropout=model_config['dropout']
        )
    elif architecture == 'hybrid_transformer':
        from models import create_hybrid_transformer
        base_model = create_hybrid_transformer(
            model_size=model_config['size'],
            in_channels=12,
            num_classes=1,
            dropout=model_config['dropout']
        )
    else:
        raise ValueError(f"Visualization only supports transformer architectures, got: {architecture}")
    
    # Load checkpoint with the base model
    lightning_model = BrugadaClassifier.load_from_checkpoint(
        args.checkpoint,
        model=base_model  # Pass the base model
    )
    
    lightning_model.eval()
    model = lightning_model.model  # Extract the base transformer
    
    # Get samples to visualize
    print("\n[5/5] Visualizing attention patterns...")
    test_loader = datamodule.test_dataloader()
    
    samples_visualized = 0
    for batch_idx, batch in enumerate(test_loader):
        for sample in batch:
            # Filter by label if requested
            if args.brugada_only and sample.label.item() == 0:
                continue
            
            # Visualize this sample
            visualize_sample(
                model,
                sample,
                args.output_dir,
                samples_visualized
            )
            
            samples_visualized += 1
            if samples_visualized >= args.num_samples:
                break
        
        if samples_visualized >= args.num_samples:
            break
    
    print("\n" + "=" * 80)
    print("âœ“ VISUALIZATION COMPLETE")
    print("=" * 80)
    print(f"\nVisualized {samples_visualized} samples")
    print(f"Output directory: {args.output_dir}")
    print("\nFiles generated per sample:")
    print("  - sample_X_layer_Y_head_0.png: Attention heatmap for layer Y")
    print("  - sample_X_rollout.png: Cumulative attention across all layers")
    print("  - sample_X_ecg_with_attention.png: ECG with attention overlay")


if __name__ == "__main__":
    main()
