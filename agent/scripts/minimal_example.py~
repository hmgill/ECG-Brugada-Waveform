#!/usr/bin/env python3
"""
Minimal Working Example: Load Brugada ECG data with MLflow and Logfire integration.

This script demonstrates:
1. Loading configuration from YAML
2. Initializing Logfire for structured logging
3. Setting up MLflow experiment tracking
4. Loading data using BrugadaDataModule
5. Logging statistics to MLflow and Logfire
"""

import sys
from pathlib import Path
import yaml

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import logfire
import mlflow
from data import DataConfig, AugmentationConfig, BrugadaDataModule
from helpers import (
    setup_logfire,
    setup_mlflow,
    log_data_statistics,
    log_config,
    log_params_from_config,
    log_dataset_info
)


def load_config(config_path: Path) -> dict:
    """Load YAML configuration file."""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def main():
    """Run minimal working example."""
    
    # Paths
    project_root = Path(__file__).parent.parent
    config_dir = project_root / "config"
    
    print("=" * 80)
    print("BRUGADA ECG CLASSIFIER - MINIMAL WORKING EXAMPLE")
    print("=" * 80)
    
    # ========================================================================
    # 1. Initialize Logging
    # ========================================================================
    print("\n[1/5] Initializing Logfire...")
    setup_logfire(
        service_name="brugada-mwe",
        log_dir=project_root / "logs",
        send_to_logfire=False  # Set to True to send to cloud
    )
    logfire.info('Starting minimal working example')
    
    # ========================================================================
    # 2. Load Configurations
    # ========================================================================
    print("\n[2/5] Loading configurations...")
    
    # Load data config
    data_config_dict = load_config(config_dir / "data.yaml")
    logfire.info('Loaded data configuration', config_path=str(config_dir / "data.yaml"))
    log_config("data", data_config_dict)
    
    # Load MLflow config
    mlflow_config = load_config(config_dir / "mlflow.yaml")
    logfire.info('Loaded MLflow configuration', config_path=str(config_dir / "mlflow.yaml"))
    
    # Create Pydantic configs
    data_config = DataConfig(**data_config_dict)
    
    # Create augmentation config from nested dict
    aug_params = data_config_dict.get('augmentation', {})
    augmentation_config = AugmentationConfig(
        amplitude_scale_range=tuple(aug_params.get('amplitude_scale', {}).get('range', [0.8, 1.2])),
        amplitude_scale_prob=aug_params.get('amplitude_scale', {}).get('prob', 0.5),
        noise_std=aug_params.get('noise', {}).get('std', 0.05),
        noise_prob=aug_params.get('noise', {}).get('prob', 0.5),
        baseline_wander_amplitude=aug_params.get('baseline_wander', {}).get('amplitude', 0.1),
        baseline_wander_frequency=tuple(aug_params.get('baseline_wander', {}).get('frequency_range', [0.1, 0.5])),
        baseline_wander_prob=aug_params.get('baseline_wander', {}).get('prob', 0.3),
        time_warp_sigma=aug_params.get('time_warp', {}).get('sigma', 0.2),
        time_warp_knots=aug_params.get('time_warp', {}).get('knots', 4),
        time_warp_prob=aug_params.get('time_warp', {}).get('prob', 0.2),
        lead_scale_range=tuple(aug_params.get('lead_scale', {}).get('range', [0.9, 1.1])),
        lead_scale_prob=aug_params.get('lead_scale', {}).get('prob', 0.3),
    )
    
    print(f"✓ Data config loaded: {data_config.metadata_path}")
    print(f"✓ Batch size: {data_config.batch_size}")
    print(f"✓ Normalization: {data_config.normalization_method}")
    
    # ========================================================================
    # 3. Setup MLflow
    # ========================================================================
    print("\n[3/5] Setting up MLflow experiment tracking...")
    
    experiment_id = setup_mlflow(
        tracking_uri=mlflow_config['tracking_uri'],
        experiment_name=mlflow_config['experiment_name']
    )
    logfire.info('MLflow experiment initialized', experiment_id=experiment_id)
    print(f"✓ MLflow experiment: {mlflow_config['experiment_name']}")
    print(f"✓ Tracking URI: {mlflow_config['tracking_uri']}")
    
    # Start MLflow run
    with mlflow.start_run(run_name=f"{mlflow_config['run_name_prefix']}-mwe") as run:
        run_id = run.info.run_id
        logfire.info('MLflow run started', run_id=run_id)
        print(f"✓ Run ID: {run_id}")
        
        # Log tags
        for key, value in mlflow_config.get('tags', {}).items():
            mlflow.set_tag(key, value)
        
        # ====================================================================
        # 4. Load Data
        # ====================================================================
        print("\n[4/5] Loading ECG data...")
        logfire.info('Initializing DataModule')
        
        datamodule = BrugadaDataModule(
            config=data_config,
            augmentation_config=augmentation_config
        )
        
        # Setup data
        datamodule.setup(stage="fit")
        logfire.info('DataModule setup complete')
        
        # Get statistics
        stats = datamodule.statistics
        stats_dict = {
            'total_samples': stats.total_samples,
            'normal_samples': stats.normal_samples,
            'brugada_samples': stats.brugada_samples,
            'class_balance_ratio': stats.class_balance_ratio,
            'pos_weight': stats.pos_weight,
            'missing_files': stats.missing_files,
        }
        
        print(f"\n✓ Dataset loaded successfully!")
        print(f"  Total samples: {stats.total_samples}")
        print(f"  Normal: {stats.normal_samples} ({stats.normal_samples/stats.total_samples*100:.1f}%)")
        print(f"  Brugada: {stats.brugada_samples} ({stats.brugada_samples/stats.total_samples*100:.1f}%)")
        print(f"  Class ratio (B:N): 1:{stats.class_balance_ratio:.2f}")
        print(f"  Pos weight for loss: {stats.pos_weight:.3f}")
        
        print(f"\n✓ Data splits:")
        print(f"  Train: {len(datamodule.train_metadata)} samples")
        print(f"  Val: {len(datamodule.val_metadata)} samples")
        print(f"  Test: {len(datamodule.test_metadata)} samples")
        
        # ====================================================================
        # 5. Log to MLflow and Logfire
        # ====================================================================
        print("\n[5/5] Logging to MLflow and Logfire...")
        
        # Log to Logfire
        log_data_statistics(stats_dict)
        
        # Log to MLflow
        log_params_from_config(data_config_dict, prefix="data.")
        log_dataset_info(stats_dict)
        
        print("✓ Statistics logged to MLflow and Logfire")
        
        # ====================================================================
        # Bonus: Inspect a batch
        # ====================================================================
        print("\n[BONUS] Inspecting sample batch...")
        
        train_loader = datamodule.train_dataloader()
        batch = next(iter(train_loader))
        
        logfire.info(
            'Sample batch loaded',
            batch_size=len(batch),
            signal_shape=str(batch[0].signal.shape),
            num_brugada=sum(s.label.item() for s in batch),
            num_normal=sum(1 - s.label.item() for s in batch)
        )
        
        sample = batch[0]
        print(f"  Batch size: {len(batch)}")
        print(f"  Sample signal shape: {sample.signal.shape}")
        print(f"  Sample label: {sample.label.item()} ({sample.label.item() and 'Brugada' or 'Normal'})")
        print(f"  Labels in batch - Normal: {sum(1-s.label.item() for s in batch)}, Brugada: {sum(s.label.item() for s in batch)}")
        
        # ====================================================================
        # Complete
        # ====================================================================
        print("\n" + "=" * 80)
        print("✓ MINIMAL WORKING EXAMPLE COMPLETE")
        print("=" * 80)
        print(f"\nMLflow UI: Run 'mlflow ui' in {project_root} and visit http://localhost:5000")
        print(f"Logs: Check {project_root / 'logs'} for Logfire logs")
        print("\nNext steps:")
        print("  1. Implement ResNet-1D model architecture")
        print("  2. Create Lightning module with training loop")
        print("  3. Run full training with scripts/train.py")
        
        logfire.info('Minimal working example completed successfully')


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logfire.error('Minimal working example failed', error=str(e), exc_info=True)
        raise
